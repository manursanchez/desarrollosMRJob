{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import mrjob\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "\n",
    "    OUTPUT_PROTOCOL = mrjob.protocol.RawProtocol\n",
    "    ndim=None\n",
    "    Centroid=None\n",
    "    nclass=None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRKmeans, self).__init__(*args, **kwargs)\n",
    "        global _global_dict\n",
    "        _global_dict = {}\n",
    "\n",
    "    \n",
    "\n",
    "    def configure_args(self):\n",
    "        \n",
    "        super(MRKmeans, self).configure_args()\n",
    "        self.add_file_arg('--centroids_input')\n",
    "        self.add_file_arg('--centroids_output')    \n",
    "        self.add_passthru_arg('--iterations', help='iterations', default=10, type=int)        \n",
    "\n",
    "    def load_args(self,args):\n",
    "       \n",
    "        super(MRKmeans,self).load_args(args)\n",
    "        \n",
    "        if self.options.centroids_input is None:\n",
    "            self.option_parser.error(\"please type the centroids input file.\")\n",
    "        else:\n",
    "            self.infile = self.options.centroids_input\n",
    "        \n",
    "        if self.options.centroids_output is None:\n",
    "            self.outfile = self.infile\n",
    "        else:\n",
    "            self.outfile = self.options.centroids_output\n",
    "        self.iterations = self.options.iterations\n",
    "\n",
    "    def get_centroids(self):\n",
    "        \n",
    "        Centroid = np.loadtxt(self.infile, delimiter = ',')\n",
    "        return Centroid\n",
    "\n",
    "    def write_centroids(self, Centroid):\n",
    "       \n",
    "        np.savetxt(\"./foo.txt\", Centroid[None], fmt = '%.5f',delimiter = ',')\n",
    " \n",
    "    def relabel_data(self, _, line):\n",
    "        '''\n",
    "        Mapper\n",
    "        '''\n",
    "        try:\n",
    "            Coord, Cluster_ID = line.split('|')\n",
    "        except:\n",
    "            Coord = line\n",
    "        Coord_arr = np.array(Coord.split(','), dtype = float)\n",
    "        global Centroid\n",
    "        Centroid = self.get_centroids()\n",
    "        Centroid_arr = np.reshape(Centroid, (-1, len(Coord_arr)))\n",
    "        global nclass\n",
    "        global ndim\n",
    "        nclass = Centroid_arr.shape[0]\n",
    "        ndim = Centroid_arr.shape[1]\n",
    "        \n",
    "        Distance = ((Centroid_arr - Coord_arr)**2).sum(axis = 1)\n",
    "        Cluster_ID = str(Distance.argmin() + 1)\n",
    "        Coord_arr = Coord_arr.tolist()\n",
    "        yield Cluster_ID, Coord_arr\n",
    "    \n",
    "    def node_combine(self, Cluster_ID, values):\n",
    "        '''\n",
    "        Combiner\n",
    "        '''\n",
    "        \n",
    "        global ndim\n",
    "        Coord_set = []\n",
    "        Coord_sum = np.zeros(ndim)\n",
    "        for Coord_arr in values:\n",
    "            Coord_set.append(','.join(str(e) for e in Coord_arr))\n",
    "            Coord_arr = np.array(Coord_arr, dtype = float)\n",
    "            Coord_sum += Coord_arr\n",
    "            Coord_sum = Coord_sum.tolist()\n",
    "        yield Cluster_ID, (Coord_sum, Coord_set)\n",
    "    \n",
    "    def update_centroid(self, Cluster_ID, values):\n",
    "        '''\n",
    "        Reducer\n",
    "        '''\n",
    "        global ndim\n",
    "        global Centroid\n",
    "        global nclass\n",
    "        final_Coord_set = []\n",
    "        n = 0\n",
    "        final_Coord_sum = np.zeros(ndim)\n",
    "        for Coord_sum, Coord_set in values:\n",
    "            final_Coord_set += Coord_set\n",
    "            Coord_sum = np.array(Coord_sum, dtype = float)\n",
    "            final_Coord_sum += Coord_sum\n",
    "            n += 1\n",
    "        \n",
    "        new_Centroid = final_Coord_sum / n\n",
    "        Centroid[ndim * (int(Cluster_ID) - 1) : ndim * int(Cluster_ID)] = new_Centroid\n",
    "        if int(Cluster_ID) == nclass:\n",
    "\t        self.write_centroids(Centroid)\n",
    "\n",
    "        for final_Coord in final_Coord_set:\n",
    "            yield None, (final_Coord + '|' + Cluster_ID)\n",
    "\n",
    "         \n",
    "    def steps(self):   \n",
    "        return [MRStep(mapper=self.relabel_data,\n",
    "                       combiner=self.node_combine,\n",
    "                       reducer=self.update_centroid)] * self.iterations \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run() \n",
    "python ./code/KMeans.py -r local \\\n",
    "--centroids_input ./test_dataset/Centroid.txt \\\n",
    "--centroids_output ./test_dataset/Centroid.txt  \\\n",
    "--iterations 10 \\\n",
    "./test_dataset/e.txt \n",
    "Probable cause of failure:\n",
    "\n",
    "+ /home/logic/anaconda3/bin/python KMeans.py --step-num=0 --combiner --centroids_input Centroid.txt --centroids_output Centroid-1.txt --iterations 10\n",
    "Traceback (most recent call last):\n",
    "  File \"KMeans.py\", line 132, in <module>\n",
    "    MRKmeans.run() \n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 616, in run\n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 678, in execute\n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 780, in run_combiner\n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 850, in combine_pairs\n",
    "  File \"/tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/wd/mrjob.zip/mrjob/job.py\", line 889, in _combine_or_reduce_pairs\n",
    "  File \"KMeans.py\", line 92, in node_combine\n",
    "    Coord_sum = np.zeros(ndim)\n",
    "NameError: name 'ndim' is not defined\n",
    "\n",
    "(from lines 7-18 of /tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/stderr)\n",
    "\n",
    "while reading input from /tmp/KMeans.logic.20200423.102930.879303/step/000/combiner/00000/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have the following structure MRJOb:\n",
    "\n",
    "def mapper1(self, _, line):\n",
    "...\n",
    "\n",
    "def reducer1(self, nodeA, nodeP):\n",
    "...\n",
    "\n",
    "def mapper2(self, nodeA, nodeP):\n",
    "...\n",
    "\n",
    "def steps(self):\n",
    "        return [MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "                MRStep(mapper=self.mapper2)]\n",
    "I execute the program with the following command:\n",
    "\n",
    "python main.py a.txt\n",
    "\n",
    "I want iterate for N times, the steps of MRJob with the same input file. How to solve? Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_args(self):\n",
    "    super(MRYourJob, self).configure_args()\n",
    "    self.add_passthru_arg(\n",
    "            '-n', '--num-iterations', default=1, type=int,\n",
    "            help='Number of times to run the job')\n",
    "\n",
    "def steps(self):\n",
    "    return [MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "                MRStep(mapper=self.mapper2)] * self.options.num_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to join between two files,but I get Error error : NameError: name 'names' is not defined\n",
    "\n",
    "!python job.py data.txt --database item.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ejemploargs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemploargs.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "class MRPeopleScores(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_1,reducer_init=self.reducer_init, reducer=self.reducer_1)\n",
    "            ]\n",
    "   \n",
    "    def configure_args(self):\n",
    "        super(MRPeopleScores, self).configure_args()\n",
    "        self.add_file_arg('--database')\n",
    "\n",
    "    def mapper_1(self, _, line):\n",
    "        (UserID, Reputation,Location) = line.split(\";\")\n",
    "        #(employee_id, age,var_,salary) = line.split(\"\\t\")\n",
    "        #yield int(employee_id), salary\n",
    "        yield UserID, Location\n",
    "        \n",
    "    def reducer_init(self):\n",
    "       \n",
    "        #with open(\"item.txt\") as f:\n",
    "        with open(\"tablaB.csv\") as f:\n",
    "            self.names=[]\n",
    "            for line in f:\n",
    "                #fields = line.split('|')\n",
    "                fields = line.split(';')\n",
    "                encontrado=re.search('[a-zA-Z]',fields[0])#Para que no tenga en cuenta las cabeceras de las tablas\n",
    "                if encontrado==None:\n",
    "                    #self.names[fields[0]] = fields[1] \n",
    "                    self.names[0] = fields[1]\n",
    "                    \n",
    "    #def reducer(self,employee_id, salary):\n",
    "    def reducer_1(self,UserID, Location):\n",
    "        \n",
    "        #for salary_ in salary:\n",
    "        for Location_ in Location:\n",
    "              yield UserID,(Location_,self.names[UserID])\n",
    "   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRPeopleScores.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\ejemploargs.mrsanchez.20200922.193455.169313\n",
      "Running step 1 of 1...\n",
      "\n",
      "Error while reading from C:\\Users\\MRSANC~1\\AppData\\Local\\Temp\\ejemploargs.mrsanchez.20200922.193455.169313\\step\\000\\reducer\\00000\\input:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"ejemploargs.py\", line 42, in <module>\n",
      "    MRPeopleScores.run()\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\runner.py\", line 507, in run\n",
      "    self._run()\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\sim.py\", line 160, in _run\n",
      "    self._run_step(step, step_num)\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\sim.py\", line 169, in _run_step\n",
      "    self._run_streaming_step(step, step_num)\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\sim.py\", line 186, in _run_streaming_step\n",
      "    self._run_reducers(step_num, num_reducer_tasks)\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\sim.py\", line 290, in _run_reducers\n",
      "    for task_num in range(num_reducer_tasks)\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\sim.py\", line 129, in _run_multiple\n",
      "    func()\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\sim.py\", line 746, in _run_task\n",
      "    stdin, stdout, stderr, wd, env)\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\inline.py\", line 132, in invoke_task\n",
      "    task.execute()\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\job.py\", line 681, in execute\n",
      "    self.run_reducer(self.options.step_num)\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\job.py\", line 795, in run_reducer\n",
      "    for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\job.py\", line 866, in reduce_pairs\n",
      "    for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n",
      "  File \"C:\\Users\\mrsanchez\\Anaconda3\\lib\\site-packages\\mrjob\\job.py\", line 880, in _combine_or_reduce_pairs\n",
      "    for k, v in task_init() or ():\n",
      "  File \"ejemploargs.py\", line 31, in reducer_init\n",
      "    self.names[1] = fields\n",
      "IndexError: list assignment index out of range\n"
     ]
    }
   ],
   "source": [
    "!python ejemploargs.py tablaA.csv --database tablaB.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
